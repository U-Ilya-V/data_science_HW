{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Алгоритм шифра Цезаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем входные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Сделаем два алфавита\n",
    "lang_alphabets = {\n",
    "    'russian': 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя ',\n",
    "    'english': 'abcdefghijklmnopqrstuvwxyz ',\n",
    "}\n",
    "\n",
    "# Создаем словари\n",
    "char_to_index = {}\n",
    "for lang, alphabet in lang_alphabets.items():\n",
    "    char_to_index[lang] = {char: idx for idx, char in enumerate(alphabet)}\n",
    "    \n",
    "index_to_char = {}\n",
    "for lang, alphabet in lang_alphabets.items():\n",
    "    index_to_char[lang] = {idx: char for idx, char in enumerate(alphabet)}\n",
    "\n",
    "# Определяим размеры входных данных и выходов\n",
    "input_layer_size = {lang: len(alphabet) for lang, alphabet in lang_alphabets.items()}\n",
    "hidden_layer_size = 128  # Определите размер скрытого слоя\n",
    "output_layer_size = {lang: len(alphabet) for lang, alphabet in lang_alphabets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем алгоритм шифра Цезаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caesar_cipher(text, shift, alphabet):\n",
    "    res = \"\"\n",
    "    for char in text:\n",
    "        if char in alphabet:\n",
    "            index = alphabet.index(char)\n",
    "            new_index = (index + shift) % len(alphabet)\n",
    "            res += alphabet[new_index]\n",
    "        else:\n",
    "            res += char \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples=5000, shift=2, language ='russian'):\n",
    "    alphabet = lang_alphabets[language]\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        length = random.randint(5, 15) \n",
    "        random_text = ''.join(random.choices(alphabet, k=length)) \n",
    "        encoded_text = caesar_cipher(random_text, shift, alphabet)\n",
    "        samples.append((encoded_text, random_text)) \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CipherDataset(Dataset):\n",
    "    def __init__(self, data, language):\n",
    "        self.data = data\n",
    "        self.language = language\n",
    "        self.char_to_index = char_to_index[self.language]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded, original = self.data[index]\n",
    "        return (\n",
    "            torch.tensor([self.char_to_index[char] for char in encoded], dtype=torch.long),\n",
    "            torch.tensor([self.char_to_index[char] for char in original], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию для добавления паддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    encoded_inputs = [item[0] for item in batch]\n",
    "    original_outputs = [item[1] for item in batch]\n",
    "    \n",
    "    max_length = max(len(seq) for seq in encoded_inputs)\n",
    "    \n",
    "    padded_encoded = torch.stack([\n",
    "        torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in encoded_inputs\n",
    "    ])\n",
    "    \n",
    "    padded_original = torch.stack([\n",
    "        torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in original_outputs\n",
    "    ])\n",
    "    \n",
    "    return padded_encoded, padded_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем архитектуру нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, 30)\n",
    "        self.rnn = nn.RNN(input_size=30, hidden_size=hidden_layer_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(x) \n",
    "        return self.fc(rnn_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем финкцию для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for encoded_x, original_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(encoded_x) \n",
    "\n",
    "            output = output.view(-1, output_layer_size[dataloader.dataset.language])\n",
    "            original_y = original_y.view(-1) \n",
    "            loss = criterion(output, original_y) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию для проверки качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, encoded_text, language):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([char_to_index[language][char] for char in encoded_text]).view(1, -1)\n",
    "        output = model(input_tensor)\n",
    "        output = output.view(-1, output_layer_size[language])\n",
    "        _, predicted_indices = torch.max(output, -1)\n",
    "        decoded_text = ''.join([index_to_char[language][idx.item()] for idx in predicted_indices])\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаемся!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 2.148571878671646\n",
      "Epoch 2/30, Loss: 1.0834589507430792\n",
      "Epoch 3/30, Loss: 0.3885561008937657\n",
      "Epoch 4/30, Loss: 0.1620003948919475\n",
      "Epoch 5/30, Loss: 0.10724074952304363\n",
      "Epoch 6/30, Loss: 0.08267764491029084\n",
      "Epoch 7/30, Loss: 0.06943792675156146\n",
      "Epoch 8/30, Loss: 0.060939543414860964\n",
      "Epoch 9/30, Loss: 0.05538130353670567\n",
      "Epoch 10/30, Loss: 0.050661571440286934\n",
      "Epoch 11/30, Loss: 0.04743930004769936\n",
      "Epoch 12/30, Loss: 0.04461652191821486\n",
      "Epoch 13/30, Loss: 0.04204148007556796\n",
      "Epoch 14/30, Loss: 0.0407789614982903\n",
      "Epoch 15/30, Loss: 0.03670441807480529\n",
      "Epoch 16/30, Loss: 0.03434006532188505\n",
      "Epoch 17/30, Loss: 0.03368434909498319\n",
      "Epoch 18/30, Loss: 0.030366720544407144\n",
      "Epoch 19/30, Loss: 0.027158699231222272\n",
      "Epoch 20/30, Loss: 0.02601560510811396\n",
      "Epoch 21/30, Loss: 0.022681642178213224\n",
      "Epoch 22/30, Loss: 0.018925343814771622\n",
      "Epoch 23/30, Loss: 0.021787123594549485\n",
      "Epoch 24/30, Loss: 0.02117867604829371\n",
      "Epoch 25/30, Loss: 0.018678835622267798\n",
      "Epoch 26/30, Loss: 0.02365209732670337\n",
      "Epoch 27/30, Loss: 0.013730822916841134\n",
      "Epoch 28/30, Loss: 0.010183661535847932\n",
      "Epoch 29/30, Loss: 0.00825311042717658\n",
      "Epoch 30/30, Loss: 0.006372440955601633\n",
      "Encoded: xrthztofsmd\n",
      "Original: vprfxrmdqkb\n",
      "Decoded: vprfxrmdqkb\n",
      "\n",
      "Encoded: gbovcvb\n",
      "Original: e mtat \n",
      "Decoded: e mtat \n",
      "\n",
      "Encoded: jvxvgfzpeexuh\n",
      "Original: htvtedxnccvsf\n",
      "Decoded: htvtedxnccvsf\n",
      "\n",
      "Encoded:  fqujgmejamo\n",
      "Original: ydoshekchzkm\n",
      "Decoded: ydoshekchakm\n",
      "\n",
      "Encoded: yhkosb\n",
      "Original: wfimq \n",
      "Decoded: wfimq \n",
      "\n",
      "Encoded: isc k\n",
      "Original: gqayi\n",
      "Decoded: gqayi\n",
      "\n",
      "Encoded: eelcfxhoove\n",
      "Original: ccjadvfmmtc\n",
      "Decoded: ccjadvfmmtc\n",
      "\n",
      "Encoded: ybqtggsgi\n",
      "Original: w oreeqeg\n",
      "Decoded: w oreeqeg\n",
      "\n",
      "Encoded: tda dspg\n",
      "Original: rbzybqne\n",
      "Decoded: rbzybqne\n",
      "\n",
      "Encoded: ods h u ytvbj\n",
      "Original: mbqyfysywrt h\n",
      "Decoded: mbqyfysywrt h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Задаем параметры\n",
    "    language = 'english'\n",
    "    num_samples = 1000\n",
    "    shift = 2\n",
    "    \n",
    "    data = generate_data(num_samples=num_samples, shift=shift, language=language)\n",
    "\n",
    "    # Подготавливаем и распределяем данные\n",
    "    dataset = CipherDataset(data, language=language)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "   # Инициализация модели\n",
    "    model = SimpleRNN(input_layer_size[language]) \n",
    "\n",
    "   # Определяем функцию потерь\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "   # Инициализируем оптимизатор\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)  \n",
    "   # Обученаем модель\n",
    "    train(model, dataloader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "    # Пример для проверки\n",
    "    test_samples = generate_data(10, shift=shift, language=language)\n",
    "    for encoded, original in test_samples:\n",
    "        decoded = evaluate(model, encoded, language)\n",
    "        print(f'Encoded: {encoded}\\nOriginal: {original}\\nDecoded: {decoded}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "Loss равномерно падал на всё процессе обучения и после 30 эпох составил 0.0063, что является очень хорошим результатом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
