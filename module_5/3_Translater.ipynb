{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Решить задачу машинного перевода\n",
    "Задание:\n",
    "- Формируем датасет с исходного языка на целевой (код прописать в классе)\n",
    "- Строим архитектуру нейронной сети \n",
    "- Обучаем \n",
    "- Проверить качество с помощью метрики BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import sacrebleu\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные (данные решил взять с сайта https://www.manythings.org/anki/, надеюсь это был законный мув :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_samples=10000):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    pairs = [line.strip().split(\"\\t\")[:2] for line in lines[:num_samples]]\n",
    "    return pairs\n",
    "\n",
    "dataset = load_dataset(\"rus-eng.txt\", num_samples=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем словари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.index2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.freq = {}\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            if word not in self.freq:\n",
    "                self.freq[word] = 1\n",
    "            else:\n",
    "                self.freq[word] += 1\n",
    "\n",
    "    def build_vocab(self, min_freq=2):\n",
    "        idx = 4\n",
    "        for word, freq in self.freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2index[word] = idx\n",
    "                self.index2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, sentence):\n",
    "        return [self.word2index.get(word, self.word2index[\"<unk>\"]) for word in sentence]\n",
    "\n",
    "ru_vocab = Vocabulary()\n",
    "en_vocab = Vocabulary()\n",
    "\n",
    "for ru, en in dataset:\n",
    "    ru_vocab.add_sentence(tokenize(ru))\n",
    "    en_vocab.add_sentence(tokenize(en))\n",
    "\n",
    "ru_vocab.build_vocab()\n",
    "en_vocab.build_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_vocab, trg_vocab):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, trg = self.data[idx]\n",
    "        src_tokens = self.src_vocab.numericalize(tokenize(src)) + [self.src_vocab.word2index[\"<eos>\"]]\n",
    "        trg_tokens = [self.trg_vocab.word2index[\"<sos>\"]] + self.trg_vocab.numericalize(tokenize(trg)) + [self.trg_vocab.word2index[\"<eos>\"]]\n",
    "        return torch.tensor(src_tokens), torch.tensor(trg_tokens)\n",
    "\n",
    "dataset = TranslationDataset(dataset, ru_vocab, en_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collate fn для динамических данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=ru_vocab.word2index[\"<pad>\"], batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=en_vocab.word2index[\"<pad>\"], batch_first=True)\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        hidden, cell = self.encoder(src)\n",
    "        batch_size, trg_len = trg.shape\n",
    "        outputs = torch.zeros(batch_size, trg_len, len(en_vocab.word2index))\n",
    "\n",
    "        input = trg[:, 0]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            input = trg[:, t] if random.random() < teacher_forcing_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1190.5804\n",
      "Epoch 2, Loss: 895.7237\n",
      "Epoch 3, Loss: 729.8688\n",
      "Epoch 4, Loss: 594.3665\n",
      "Epoch 5, Loss: 484.0489\n",
      "Epoch 6, Loss: 396.5794\n",
      "Epoch 7, Loss: 326.8826\n",
      "Epoch 8, Loss: 279.7658\n",
      "Epoch 9, Loss: 248.8953\n",
      "Epoch 10, Loss: 231.3638\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(ru_vocab.word2index)\n",
    "output_dim = len(en_vocab.word2index)\n",
    "emb_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "\n",
    "encoder = Encoder(input_dim, emb_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(output_dim, emb_dim, hidden_dim, num_layers)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab.word2index[\"<pad>\"])\n",
    "\n",
    "def train(model, loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for src, trg in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "train(model, train_loader, optimizer, criterion, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 100.00\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bleu(model, loader):\n",
    "    model.eval()\n",
    "    actual, predicted = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in loader:\n",
    "            hidden, cell = model.encoder(src)\n",
    "            batch_size = src.shape[0]\n",
    "            input = torch.tensor([en_vocab.word2index[\"<sos>\"]]).repeat(batch_size).to(src.device)\n",
    "\n",
    "            translated_sentences = [[] for _ in range(batch_size)]\n",
    "\n",
    "            for _ in range(50): \n",
    "                output, hidden, cell = model.decoder(input, hidden, cell)\n",
    "                best_guess = output.argmax(1)  \n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    translated_sentences[i].append(best_guess[i].item())  \n",
    "                \n",
    "                input = best_guess  \n",
    "\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                actual_sentence = \" \".join([en_vocab.index2word[idx.item()] for idx in trg[i] if idx.item() not in {en_vocab.word2index[\"<pad>\"], en_vocab.word2index[\"<sos>\"], en_vocab.word2index[\"<eos>\"]}])\n",
    "                predicted_sentence = \" \".join([en_vocab.index2word[idx] for idx in translated_sentences[i] if idx not in {en_vocab.word2index[\"<pad>\"], en_vocab.word2index[\"<sos>\"], en_vocab.word2index[\"<eos>\"]}])\n",
    "                \n",
    "                actual.append([actual_sentence])  \n",
    "                predicted.append(predicted_sentence)\n",
    "\n",
    "    bleu_score = sacrebleu.corpus_bleu(predicted, actual)\n",
    "    print(f\"BLEU Score: {bleu_score.score:.2f}\")\n",
    "\n",
    "evaluate_bleu(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "\n",
    "Loss равномерно падал на все процессе обучения, а BLEU показал 100%... Всё получилось :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
