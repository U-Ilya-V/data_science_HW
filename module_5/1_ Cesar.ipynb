{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Алгоритм шифра Цезаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем входные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Сделаем два алфавита\n",
    "ALPHABETS = {\n",
    "    'russian': 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя ',\n",
    "    'english': 'abcdefghijklmnopqrstuvwxyz ',\n",
    "}\n",
    "\n",
    "# Создаем словари\n",
    "CHAR_TO_INDEX = {lang: {char: idx for idx, char in enumerate(alphabet)} for lang, alphabet in ALPHABETS.items()}\n",
    "INDEX_TO_CHAR = {lang: {idx: char for idx, char in enumerate(alphabet)} for lang, alphabet in ALPHABETS.items()}\n",
    "\n",
    "# Определяим размеры входных данных и выходов\n",
    "INPUT_SIZES = {lang: len(alphabet) for lang, alphabet in ALPHABETS.items()}\n",
    "HIDDEN_SIZE = 128  # Определите размер скрытого слоя\n",
    "OUTPUT_SIZES = {lang: len(alphabet) for lang, alphabet in ALPHABETS.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем алгоритм шифра Цезаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caesar_cipher(text, shift, alphabet):\n",
    "    result = \"\"\n",
    "    for char in text:\n",
    "        if char in alphabet:\n",
    "            index = alphabet.index(char)\n",
    "\n",
    "            new_index = (index + shift) % len(alphabet)\n",
    "            result += alphabet[new_index]\n",
    "        else:\n",
    "            result += char \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples=1000, shift=2, language ='russian'):\n",
    "    alphabet = ALPHABETS[language]\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        length = random.randint(5, 15) \n",
    "        random_text = ''.join(random.choices(alphabet, k=length)) \n",
    "        encoded_text = caesar_cipher(random_text, shift, alphabet)\n",
    "        samples.append((encoded_text, random_text)) \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CipherDataset(Dataset):\n",
    "    def __init__(self, data, language):\n",
    "        if language not in ALPHABETS:\n",
    "            raise ValueError(\"Unsupported language. Please choose 'russian', 'english', or 'german'.\")\n",
    "        \n",
    "        self.data = data\n",
    "        self.language = language\n",
    "        self.char_to_index = CHAR_TO_INDEX[self.language]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded, original = self.data[index]\n",
    "        return (\n",
    "            torch.tensor([self.char_to_index[char] for char in encoded], dtype=torch.long),\n",
    "            torch.tensor([self.char_to_index[char] for char in original], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию для добавления паддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    encoded_inputs = [item[0] for item in batch]\n",
    "    original_outputs = [item[1] for item in batch]\n",
    "    \n",
    "    max_length = max(len(seq) for seq in encoded_inputs)\n",
    "    \n",
    "    padded_encoded = torch.stack([\n",
    "        torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in encoded_inputs\n",
    "    ])\n",
    "    \n",
    "    padded_original = torch.stack([\n",
    "        torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in original_outputs\n",
    "    ])\n",
    "    \n",
    "    return padded_encoded, padded_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем архитектуру нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, 30)\n",
    "        self.rnn = nn.RNN(input_size=30, hidden_size=HIDDEN_SIZE, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(x) \n",
    "        return self.fc(rnn_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем финкцию для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for encoded_x, original_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(encoded_x) \n",
    "\n",
    "            output = output.view(-1, OUTPUT_SIZES[dataloader.dataset.language])\n",
    "            original_y = original_y.view(-1) \n",
    "            loss = criterion(output, original_y) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию для проверки качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, encoded_text, language):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([CHAR_TO_INDEX[language][char] for char in encoded_text]).view(1, -1)\n",
    "        output = model(input_tensor)\n",
    "        output = output.view(-1, OUTPUT_SIZES[language])\n",
    "        _, predicted_indices = torch.max(output, -1)\n",
    "        decoded_text = ''.join([INDEX_TO_CHAR[language][idx.item()] for idx in predicted_indices])\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаемся!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 2.185536801815033\n",
      "Epoch 2/30, Loss: 1.0692739002406597\n",
      "Epoch 3/30, Loss: 0.3874638080596924\n",
      "Epoch 4/30, Loss: 0.16363327228464186\n",
      "Epoch 5/30, Loss: 0.1048462565522641\n",
      "Epoch 6/30, Loss: 0.08086013910360634\n",
      "Epoch 7/30, Loss: 0.06718504603486508\n",
      "Epoch 8/30, Loss: 0.058372288825921714\n",
      "Epoch 9/30, Loss: 0.054983965354040265\n",
      "Epoch 10/30, Loss: 0.0472615328617394\n",
      "Epoch 11/30, Loss: 0.044399083184544\n",
      "Epoch 12/30, Loss: 0.04354273801436648\n",
      "Epoch 13/30, Loss: 0.04164442984620109\n",
      "Epoch 14/30, Loss: 0.036947177606634796\n",
      "Epoch 15/30, Loss: 0.036052310664672405\n",
      "Epoch 16/30, Loss: 0.032943352474831045\n",
      "Epoch 17/30, Loss: 0.029630609205923975\n",
      "Epoch 18/30, Loss: 0.02846706355921924\n",
      "Epoch 19/30, Loss: 0.028252124349819496\n",
      "Epoch 20/30, Loss: 0.028368386148940772\n",
      "Epoch 21/30, Loss: 0.02170727313205134\n",
      "Epoch 22/30, Loss: 0.018823354854248464\n",
      "Epoch 23/30, Loss: 0.017165622062748298\n",
      "Epoch 24/30, Loss: 0.015767399134347215\n",
      "Epoch 25/30, Loss: 0.013492577025317587\n",
      "Epoch 26/30, Loss: 0.014744395462912507\n",
      "Epoch 27/30, Loss: 0.011071432687458582\n",
      "Epoch 28/30, Loss: 0.009069261490367353\n",
      "Epoch 29/30, Loss: 0.008274674706626683\n",
      "Epoch 30/30, Loss: 0.00808047509053722\n",
      "Encoded: iwbododenc\n",
      "Original: gu mbmbcla\n",
      "Decoded: gu mbmbcla\n",
      "\n",
      "Encoded: cxgoyms\n",
      "Original: avemwkq\n",
      "Decoded: avemwkq\n",
      "\n",
      "Encoded: kqlmbkkqi w\n",
      "Original: iojk iiogyu\n",
      "Decoded: iojk iiogyu\n",
      "\n",
      "Encoded: uc ycdejt\n",
      "Original: saywabchr\n",
      "Decoded: saywabchr\n",
      "\n",
      "Encoded: hnhpmvx\n",
      "Original: flfnktv\n",
      "Decoded: flfnktv\n",
      "\n",
      "Encoded: lhdzcuytxcjdh\n",
      "Original: jfbxaswrvahbf\n",
      "Decoded: jfbxaswrvahbf\n",
      "\n",
      "Encoded: efiht\n",
      "Original: cdgfr\n",
      "Decoded: cdgfr\n",
      "\n",
      "Encoded: rkx yulnxmzqk\n",
      "Original: pivywsjlvkxoi\n",
      "Decoded: pivywsjlvkxoi\n",
      "\n",
      "Encoded: nejlbj\n",
      "Original: lchj h\n",
      "Decoded: lchj h\n",
      "\n",
      "Encoded: nscbzeaclvrpmq\n",
      "Original: lqa xczajtpnko\n",
      "Decoded: lqa xcaajtpnko\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Задаем параметры\n",
    "    language = 'english'\n",
    "    num_samples = 1000\n",
    "    shift = 2\n",
    "    \n",
    "    data = generate_data(num_samples=num_samples, shift=shift, language=language)\n",
    "\n",
    "    # Подготавливаем и распределяем данные\n",
    "    dataset = CipherDataset(data, language=language)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "   # Инициализация модели\n",
    "    model = SimpleRNN(INPUT_SIZES[language]) \n",
    "\n",
    "   # Определяем функцию потерь\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "   # Инициализируем оптимизатор\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)  \n",
    "   # Обученаем модель\n",
    "    train(model, dataloader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "    # Пример для проверки\n",
    "    test_samples = generate_data(10, shift=shift, language=language)\n",
    "    for encoded, original in test_samples:\n",
    "        decoded = evaluate(model, encoded, language)\n",
    "        print(f'Encoded: {encoded}\\nOriginal: {original}\\nDecoded: {decoded}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "Loss равномерно падал на всё процессе обучения и после 30 эпох составил 0.013, что является очень хорошим результатом."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
